---
title: "Final Report: Regression Problem"
subtitle: |
  | Regression Problem 
  | Data Science 3 with R (STAT 301-3)
author: "Reed Malcolm"
pagetitle: "Regression Probelm Reed Malcolm"
date: today
format:
  html:
    
    toc: true
    embed-resources: true
execute:
  warning: false
from: markdown+emoji 
reference-location: margin
citation-location: margin
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: load-data-and-packages
#| echo: false

# loading packages
library(tidyverse)
library(here)

# loading objects

```


::: {.callout-tip icon="false"}
## Github Repo Link

[Reed's Regression Problem GitHub
Repo](https://github.com/stat301-3-2025-spring/reg-pred-prob-malcor21.git)
:::

## Introduction

Price prediction using machine learning has become increasingly prominent in the rental 
space due to the prevalence of rate-recommendation platforms like RealPage.[^1] 
Machine learning is particularly amenable to rental properties on the basis of 
their wealth of physical and administrative qualities to guide prediction. A 
timeshare service like Airbnb could utilize such methods, for example, to advise 
hosts’ pricing decisions based on listing characteristics. 

This regression exercise seeks to construct exactly that type of model, using machine learning workflows to predict Airbnb rental prices based on physical and demographic qualities. As part of a coursewide data science competition, this prediction problem adheres to the model building and submission guidelines listed on the corresponding Kaggle page. Airbnb listing data for the exercise was also provided on Kaggle.[^2] 

The Airbnb regression prediction problem seeks to forecast price using machine 
learning workflows from the Stat 301 course sequence. This repository is structured
by workflow, with each pipeline constituting a wave of model submissions to the Kaggle
competition. Workflows consist of all predictive modeling steps save for data cleaning---variable
selection, preprocessing, model fitting and tuning, evaluation, and final fitting.
A variety of model types were employed including generalized linear, random
forest, boosted trees, k-nearest neighbors, and neural network workflows. Submission
pipelines were constructed until the A grade threshold, a mean absolute error (MAE) of
97 or less, was achieved by a generated model. This report examines two models
from the project: a boosted trees workflow that exceeded the A threshold, and a random
forest model to discuss robustness.

[^1]: Vegari, A. and Anderson, C (2025), [RealPage Opens New Front in Algorithmic Pricing Challenges with Suit Challenging Berkeley Ordinance](https://www.pbwt.com/antitrust-update-blog/realpage-opens-new-front-in-algorithmic-pricing-challenges-with-suit-challenging-berkeley-ordinance)

[^2]: Arend Kuyper (2025), [Regression (Spring 2025): Airbnb prices](https://www.kaggle.com/competitions/regression-spring-2025-airbnb-prices/overview)


## Selected Model 1

The first selected model was `rf1_11_predict.csv`, the strongest random forest model generated
during the first submission wave.

![Submission Set 1 Models, Ranked](figures/s1_autoplot.jpg){#fig-s1-autoplot}

The first submission set took a broad approach to predicting price. After implementing
the basic data cleaning steps for information extraction discussed in class---capturing 
common amenities, bathrooms per listing, etc.---I utilized a kitchen-sink philosophy
for tuning and preprocessing. Variable selection was conducted using a minimal-processing
random forest model, retrieving an arbitrary 23 predictors for use in prediction.
Preprocessing objects were generated for the five model types mentioned above utilizing only
the required steps, and parameter values largely applied default ranges when tuning.
The best models of each workflow type were advanced to the public Kaggle leaderboard
for evaluation; the submitted models performed largely on par with their training
results.

```{r}
#| label: tbl-s1
#| echo: false
#| tbl-cap: "Submission 1 Optimal Model Results"

tribble(
  ~Model_Type, ~Optimal_Model_Name, ~Testing_MAE,
  "Boosted trees", "bt1_096_predict.csv", 104.55,
  "Random forest", "rf1_11_predict.csv", 105.55,
  "K-nearest neighbors", "knn1_6_predict.csv", 117.97,
  "Neural network", "mlp1_2_predict.csv", 137.25,
  "Generalized linear model", "lasso1_1_predict.csv", 149.39
) %>% 
  knitr::kable()
```


As shown in @tbl-s1 above, the optimal boosted trees model proved the strongest of the
first wave submissions. Further boosted trees tuning was therefore pursued in the
later pipelines. However, the best random forest model finished not far behind the
boosted tree. I select `rf1_11_predict.csv`, the best-performing non-boosted trees model tested
during the exercise, as a benchmark for private leaderboard evaluation.

While the boosted trees workflow underwent extensive tuning during the subsequent
two submission pipelines, the first wave's optimal random forest model performed respectably 
with little to no bespoke adjustments. I am therefore interested to compare `rf1_11_predict.csv`'s
private leaderboard power to that of the second selected model, which was likely 
overfitted to the public leaderboard on account of its extensive parameter tuning.
Even if the first wave's random forest model does not strictly outperform its boosted
trees counterpart, a low relative MAE could demonstrate that the former is more generalizable
to foreign Airbnb data and thus worth exploring further in the future.

## Selected Model 2

The second selected model, `bt1_07_all_predict.csv`, crossed the A grade threshold
with a MAE of 96.7 on the public leaderboard. It was the best-performing boosted
trees workflow from the third submission wave.


s2:
. var select bad from class
. a little preprocessing
. lots of tuning (increased trees, learn rate, light gbm)

s3: same but with further tuning of tree_depth, trees, and loss_reduction







```{r}
#| label: tbl-s3-s4
#| echo: false
#| tbl-cap: "Submission 3 and 4 Optimal Model Results"
#| eval: false

tribble(
  ~Submission, ~Optimal_Model_Type, ~Optimal_Model_Name, ~Training_ROC_AUC, ~Testing_ROC_AUC,
  3, "Boosted trees", "bt1_23_lassovars_predict_prob.csv", 0.973, 0.972,
  4, "Boosted trees", "bt1_23_lassovars_predict_prob.csv", 0.973, 0.972
) %>% 
  knitr::kable()
```

During the third and fourth submission pipelines, I sought to improve predictive power using preprocessing and tuning levers. For the third submission, I added predictors for more potentially useful amenities, generated count variables for amenities and host verifications, and created a foreign ownership indicator. For the fourth I tried removing all amenity indicators based on discussion from class. In both cases, neither of these preprocessing strategies seemed to improve performance, with the optimal boosted trees models from the third and fourth submissions falling behind the second in terms of ROC AUC.

![Submission 4 boosted trees tuning](figures/s4_bt_autoplot.jpg){#fig-s4-autoplot}

However, I did still gather relevant tuning information from the two pipelines. During the third submission wave, I began tuning two additional hyperparameters within the `lightgbm` specification framework: tree depth, which determines the model’s level of fitting to the training data, and the minimum loss reduction. Tuning these additional parameters should have strictly improved model performance regardless of overall weaker predictions, so I carried the practice over to my fifth submission pipeline. The above @fig-s4-autoplot shows the 
tuning results that I applied to the fifth submission's successful boosted trees workflow.

The fifth submission wave returned to the variable selection and preprocessing methods from the second, which had performed best on the public leaderboard, but narrowed tuning according to the previous two instances. My second selected model, a `lightgbm` boosted trees model with lasso-selected variables, returned ROC AUC values of 0.9746 during internal model evaluation and 0.9752 on the public Kaggle leaderboard, setting a new record and breaking the 97.5 threshold.

## Conclusion

Given our past course projects and class discussions, it was not surprising that the boosted trees workflow produced the best-performing models on the public Kaggle leaderboard. Along the way, enhanced cleaning and preprocessing to extract information from the dataset as well as bespoke hyperparameter tuning were key to increasing predictive power. The second submission model made full use of those levers; the first selected model, taken from the second submission pipeline, represented a less fitted but still strong alternative.

```{r}
#| label: tbl-ranges
#| tbl-cap: "Bespoke tuning might have caused overfitting among the optimal boosted trees models"
#| echo: FALSE

tribble(
  ~Hyperparameter, ~Default_Range, ~Optimal_Public_Value,
  "learn_rate", "[-10, -1]", "-1.143",
  "tree_depth", "[1, 15]", "19"
) %>% 
  knitr::kable()
```


It is nevertheless likely that both of the selected models were overfitted to the public leaderboard’s testing set. As we have discussed, the boosted trees workflow might be generally less robust to foreign data than its peers due to its learning nature. Further, the selected models demonstrated substantially higher learn rates and tree depth values than the `tidymodels` package defaults, suggesting possible overfitting to our data sample. 
As shown in @tbl-ranges, the learn rate and tree depth parameters---which drive
boosted trees model learning---used values in Selected Model 2 that were either near or above
the `tidymodels` default range maxima. This suggests that some of Selected Model 2's
gains could have reflected overtuning and would not necessarily be generalizable
to new data.

Future classification exercises might therefore seek to improve robustness by way of ensemble modeling. Though not a necessary step to reach the public leaderboard benchmark, ensemble modeling of a powerful boosted trees workflow alongside other models, 
such as support vector machines and neural networks, would have likely produced a model with greater
external validity to the private leaderboard and future Airbnb datasets.

## AI statement

Generative AI resources (Chat GPT-4o and perplexity.ai) to generate helper code 
during the project’s data cleaning process. Specifically, I used Generative AI to produce
complex regular expressions and generate functions for cleaning amenities data. 
